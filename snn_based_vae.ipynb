{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Set Up"
      ],
      "metadata": {
        "id": "ar2rksfvOVho"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "asV86I9iyIHX"
      },
      "outputs": [],
      "source": [
        "# Connect to Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-alScVnKPlQy"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import os\n",
        "import os.path\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create save folder if it doesn't exist\n",
        "save_folder = 'location_to_save'\n",
        "if not os.path.exists(save_folder):\n",
        "    os.makedirs(save_folder)"
      ],
      "metadata": {
        "id": "m_HdEmGFPO5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoALbmmpLVrH"
      },
      "source": [
        "# Set Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFd89-kEK5WU"
      },
      "outputs": [],
      "source": [
        "epochs = 150\n",
        "batch_size = 32\n",
        "n_steps = 16 # timestep\n",
        "in_channels = 1\n",
        "lr = 0.001\n",
        "latent_dim = 35\n",
        "input_size = 40\n",
        "k = 20 # multiplier of channel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBzC01iGxqi9"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mGEAEDZPxpoy"
      },
      "outputs": [],
      "source": [
        "# Load and normalise data\n",
        "raw = np.load('location_of_npy_file')\n",
        "data_norm = np.zeros_like(raw)\n",
        "\n",
        "for i in range(len(raw)):\n",
        "    min_val = raw[i].min()\n",
        "    max_val = raw[i].max()\n",
        "    data_norm[i] = (raw[i] - min_val) / (max_val - min_val)\n",
        "\n",
        "data_norm_reshape = data_norm.reshape(-1, 1, raw.shape[1], raw.shape[2]).astype(np.float32)\n",
        "\n",
        "# Split data\n",
        "data_train, data_test = train_test_split(data_norm_reshape, test_size=0.3, random_state=42, shuffle=True)\n",
        "data_test, data_val = train_test_split(data_test, test_size=0.5, random_state=42, shuffle=True)\n",
        "\n",
        "train_loader = DataLoader(data_train, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(data_val, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(data_test, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxvdCjnVPrqr"
      },
      "source": [
        "# SNN-based VAE Function Definitions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gHBqGR0SD7Y"
      },
      "source": [
        "## SNN layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-d9EWl6RpNI"
      },
      "outputs": [],
      "source": [
        "# SNN layers\n",
        "\n",
        "dt = 5\n",
        "a = 0.25\n",
        "aa = 0.5\n",
        "Vth = 0.2\n",
        "tau = 0.25\n",
        "\n",
        "\n",
        "class SpikeAct(torch.autograd.Function):\n",
        "    \"\"\"\n",
        "        Implementation of the spiking activation function with an approximation of gradient.\n",
        "    \"\"\"\n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        ctx.save_for_backward(input)\n",
        "        # if input = u > Vth then output = 1\n",
        "        output = torch.gt(input, Vth)\n",
        "        return output.float()\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        input, = ctx.saved_tensors\n",
        "        grad_input = grad_output.clone()\n",
        "        # hu is an approximate func of df/du\n",
        "        hu = abs(input) < aa\n",
        "        hu = hu.float() / (2 * aa)\n",
        "        return grad_input * hu\n",
        "\n",
        "class LIFSpike(nn.Module):\n",
        "    \"\"\"\n",
        "        Generates spikes based on LIF module. It can be considered as an activation function and is used similar to ReLU. The input tensor needs to have an additional time dimension, which in this case is on the last dimension of the data.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(LIFSpike, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        nsteps = x.shape[-1]\n",
        "        u   = torch.zeros(x.shape[:-1] , device=x.device)\n",
        "        out = torch.zeros(x.shape, device=x.device)\n",
        "        for step in range(nsteps):\n",
        "            u, out[..., step] = self.state_update(u, out[..., max(step-1, 0)], x[..., step])\n",
        "        return out\n",
        "\n",
        "    def state_update(self, u_t_n1, o_t_n1, W_mul_o_t1_n, tau=tau):\n",
        "        u_t1_n1 = tau * u_t_n1 * (1 - o_t_n1) + W_mul_o_t1_n\n",
        "        o_t1_n1 = SpikeAct.apply(u_t1_n1)\n",
        "        return u_t1_n1, o_t1_n1\n",
        "\n",
        "class tdLinear(nn.Linear):\n",
        "    def __init__(self,\n",
        "                in_features,\n",
        "                out_features,\n",
        "                bias=True,\n",
        "                bn=None,\n",
        "                spike=None):\n",
        "        assert type(in_features) == int, 'inFeatures should not be more than 1 dimesnion. It was: {}'.format(in_features.shape)\n",
        "        assert type(out_features) == int, 'outFeatures should not be more than 1 dimesnion. It was: {}'.format(out_features.shape)\n",
        "\n",
        "        super(tdLinear, self).__init__(in_features, out_features, bias=bias)\n",
        "\n",
        "        self.bn = bn\n",
        "        self.spike = spike\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x : (N,C,T)\n",
        "        \"\"\"\n",
        "        x = x.transpose(1, 2) # (N, T, C)\n",
        "        y = F.linear(x, self.weight, self.bias)\n",
        "        y = y.transpose(1, 2)# (N, C, T)\n",
        "\n",
        "        if self.bn is not None:\n",
        "            y = y[:,:,None,None,:]\n",
        "            y = self.bn(y)\n",
        "            y = y[:,:,0,0,:]\n",
        "        if self.spike is not None:\n",
        "            y = self.spike(y)\n",
        "        return y\n",
        "\n",
        "class tdConv(nn.Conv3d):\n",
        "    def __init__(self,\n",
        "                in_channels,\n",
        "                out_channels,\n",
        "                kernel_size,\n",
        "                stride=1,\n",
        "                padding=0,\n",
        "                dilation=1,\n",
        "                groups=1,\n",
        "                bias=True,\n",
        "                bn=None,\n",
        "                spike=None,\n",
        "                is_first_conv=False):\n",
        "\n",
        "        # kernel\n",
        "        if type(kernel_size) == int:\n",
        "            kernel = (kernel_size, kernel_size, 1)\n",
        "        elif len(kernel_size) == 2:\n",
        "            kernel = (kernel_size[0], kernel_size[1], 1)\n",
        "        else:\n",
        "            raise Exception('kernelSize can only be of 1 or 2 dimension. It was: {}'.format(kernel_size.shape))\n",
        "\n",
        "        # stride\n",
        "        if type(stride) == int:\n",
        "            stride = (stride, stride, 1)\n",
        "        elif len(stride) == 2:\n",
        "            stride = (stride[0], stride[1], 1)\n",
        "        else:\n",
        "            raise Exception('stride can be either int or tuple of size 2. It was: {}'.format(stride.shape))\n",
        "\n",
        "        # padding\n",
        "        if type(padding) == int:\n",
        "            padding = (padding, padding, 0)\n",
        "        elif len(padding) == 2:\n",
        "            padding = (padding[0], padding[1], 0)\n",
        "        else:\n",
        "            raise Exception('padding can be either int or tuple of size 2. It was: {}'.format(padding.shape))\n",
        "\n",
        "        # dilation\n",
        "        if type(dilation) == int:\n",
        "            dilation = (dilation, dilation, 1)\n",
        "        elif len(dilation) == 2:\n",
        "            dilation = (dilation[0], dilation[1], 1)\n",
        "        else:\n",
        "            raise Exception('dilation can be either int or tuple of size 2. It was: {}'.format(dilation.shape))\n",
        "\n",
        "        super(tdConv, self).__init__(in_channels, out_channels, kernel, stride, padding, dilation, groups,\n",
        "                                        bias=bias)\n",
        "        self.bn = bn\n",
        "        self.spike = spike\n",
        "        self.is_first_conv = is_first_conv\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.conv3d(x, self.weight, self.bias,\n",
        "                        self.stride, self.padding, self.dilation, self.groups)\n",
        "        if self.bn is not None:\n",
        "            x = self.bn(x)\n",
        "        if self.spike is not None:\n",
        "            x = self.spike(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class tdConvTranspose(nn.ConvTranspose3d):\n",
        "    def __init__(self,\n",
        "                in_channels,\n",
        "                out_channels,\n",
        "                kernel_size,\n",
        "                stride=1,\n",
        "                padding=0,\n",
        "                output_padding=0,\n",
        "                dilation=1,\n",
        "                groups=1,\n",
        "                bias=True,\n",
        "                bn=None,\n",
        "                spike=None):\n",
        "\n",
        "        # kernel\n",
        "        if type(kernel_size) == int:\n",
        "            kernel = (kernel_size, kernel_size, 1)\n",
        "        elif len(kernel_size) == 2:\n",
        "            kernel = (kernel_size[0], kernel_size[1], 1)\n",
        "        else:\n",
        "            raise Exception('kernelSize can only be of 1 or 2 dimension. It was: {}'.format(kernel_size.shape))\n",
        "\n",
        "        # stride\n",
        "        if type(stride) == int:\n",
        "            stride = (stride, stride, 1)\n",
        "        elif len(stride) == 2:\n",
        "            stride = (stride[0], stride[1], 1)\n",
        "        else:\n",
        "            raise Exception('stride can be either int or tuple of size 2. It was: {}'.format(stride.shape))\n",
        "\n",
        "        # padding\n",
        "        if type(padding) == int:\n",
        "            padding = (padding, padding, 0)\n",
        "        elif len(padding) == 2:\n",
        "            padding = (padding[0], padding[1], 0)\n",
        "        else:\n",
        "            raise Exception('padding can be either int or tuple of size 2. It was: {}'.format(padding.shape))\n",
        "\n",
        "        # dilation\n",
        "        if type(dilation) == int:\n",
        "            dilation = (dilation, dilation, 1)\n",
        "        elif len(dilation) == 2:\n",
        "            dilation = (dilation[0], dilation[1], 1)\n",
        "        else:\n",
        "            raise Exception('dilation can be either int or tuple of size 2. It was: {}'.format(dilation.shape))\n",
        "\n",
        "\n",
        "        # output padding\n",
        "        if type(output_padding) == int:\n",
        "            output_padding = (output_padding, output_padding, 0)\n",
        "        elif len(output_padding) == 2:\n",
        "            output_padding = (output_padding[0], output_padding[1], 0)\n",
        "        else:\n",
        "            raise Exception('output_padding can be either int or tuple of size 2. It was: {}'.format(padding.shape))\n",
        "\n",
        "        super().__init__(in_channels, out_channels, kernel, stride, padding, output_padding, groups,\n",
        "                                        bias=bias, dilation=dilation)\n",
        "\n",
        "        self.bn = bn\n",
        "        self.spike = spike\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.conv_transpose3d(x, self.weight, self.bias,\n",
        "                        self.stride, self.padding,\n",
        "                        self.output_padding, self.groups, self.dilation)\n",
        "\n",
        "        if self.bn is not None:\n",
        "            x = self.bn(x)\n",
        "        if self.spike is not None:\n",
        "            x = self.spike(x)\n",
        "        return x\n",
        "\n",
        "class tdBatchNorm(nn.BatchNorm2d):\n",
        "    \"\"\"\n",
        "        Implementation of tdBN. Link to related paper: https://arxiv.org/pdf/2011.05280. In short it is averaged over the time domain as well when doing BN.\n",
        "    Args:\n",
        "        num_features (int): same with nn.BatchNorm2d\n",
        "        eps (float): same with nn.BatchNorm2d\n",
        "        momentum (float): same with nn.BatchNorm2d\n",
        "        alpha (float): an addtional parameter which may change in resblock.\n",
        "        affine (bool): same with nn.BatchNorm2d\n",
        "        track_running_stats (bool): same with nn.BatchNorm2d\n",
        "    \"\"\"\n",
        "    def __init__(self, num_features, eps=1e-05, momentum=0.1, alpha=1, affine=True, track_running_stats=True):\n",
        "        super(tdBatchNorm, self).__init__(\n",
        "            num_features, eps, momentum, affine, track_running_stats)\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def forward(self, input):\n",
        "        exponential_average_factor = 0.0\n",
        "\n",
        "        if self.training and self.track_running_stats:\n",
        "            if self.num_batches_tracked is not None:\n",
        "                self.num_batches_tracked += 1\n",
        "                if self.momentum is None:  # use cumulative moving average\n",
        "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
        "                else:  # use exponential moving average\n",
        "                    exponential_average_factor = self.momentum\n",
        "\n",
        "        # calculate running estimates\n",
        "        if self.training:\n",
        "            mean = input.mean([0, 2, 3, 4])\n",
        "            # use biased var in train\n",
        "            var = input.var([0, 2, 3, 4], unbiased=False)\n",
        "            n = input.numel() / input.size(1)\n",
        "            with torch.no_grad():\n",
        "                self.running_mean = exponential_average_factor * mean\\\n",
        "                    + (1 - exponential_average_factor) * self.running_mean\n",
        "                # update running_var with unbiased var\n",
        "                self.running_var = exponential_average_factor * var * n / (n - 1)\\\n",
        "                    + (1 - exponential_average_factor) * self.running_var\n",
        "        else:\n",
        "            mean = self.running_mean\n",
        "            var = self.running_var\n",
        "\n",
        "        input = self.alpha * Vth * (input - mean[None, :, None, None, None]) / (torch.sqrt(var[None, :, None, None, None] + self.eps))\n",
        "        if self.affine:\n",
        "            input = input * self.weight[None, :, None, None, None] + self.bias[None, :, None, None, None]\n",
        "\n",
        "        return input\n",
        "\n",
        "\n",
        "class PSP(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.tau_s = 2\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        inputs: (N, C, T)\n",
        "        \"\"\"\n",
        "        syns = None\n",
        "        syn = 0\n",
        "        n_steps = inputs.shape[-1]\n",
        "        for t in range(n_steps):\n",
        "            syn = syn + (inputs[...,t] - syn) / self.tau_s\n",
        "            if syns is None:\n",
        "                syns = syn.unsqueeze(-1)\n",
        "            else:\n",
        "                syns = torch.cat([syns, syn.unsqueeze(-1)], dim=-1)\n",
        "\n",
        "        return syns\n",
        "\n",
        "class MembraneOutputLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    outputs the last time membrane potential of the LIF neuron with V_th=infty\n",
        "    \"\"\"\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "        # n_steps = glv.n_steps\n",
        "\n",
        "        arr = torch.arange(n_steps-1,-1,-1)\n",
        "        self.register_buffer(\"coef\", torch.pow(0.8, arr)[None,None,None,None,:]) # (1,1,1,1,T)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x : (N,C,H,W,T)\n",
        "        \"\"\"\n",
        "        out = torch.sum(x*self.coef, dim=-1)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpdU7-7JShsX"
      },
      "source": [
        "## FSVAE posterior"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50pXY7mCS4Sf"
      },
      "outputs": [],
      "source": [
        "class PosteriorBernoulliSTBP(nn.Module):\n",
        "    def __init__(self, latent_dim=20, k=20, n_steps=10) -> None:\n",
        "        \"\"\"\n",
        "        modeling of q(z_t | x_<=t, z_<t)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.channels = latent_dim\n",
        "        self.k = k\n",
        "        self.n_steps = n_steps\n",
        "\n",
        "        self.layers = nn.Sequential(\n",
        "            tdLinear(self.channels*2,\n",
        "                    self.channels*2,\n",
        "                    bias=True,\n",
        "                    bn=tdBatchNorm(self.channels*2, alpha=2),\n",
        "                    spike=LIFSpike()),\n",
        "            tdLinear(self.channels*2,\n",
        "                    self.channels*4,\n",
        "                    bias=True,\n",
        "                    bn=tdBatchNorm(self.channels*4, alpha=2),\n",
        "                    spike=LIFSpike()),\n",
        "            tdLinear(self.channels*4,\n",
        "                    self.channels*k,\n",
        "                    bias=True,\n",
        "                    bn=tdBatchNorm(self.channels*k, alpha=2),\n",
        "                    spike=LIFSpike())\n",
        "        )\n",
        "        self.register_buffer('initial_input', torch.zeros(1, self.channels, 1)) # (1,C,1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        input:\n",
        "            x:(B,C,T)\n",
        "        returns:\n",
        "            sampled_z:(B,C,T)\n",
        "            q_z: (B,C,k,T) # indicates q(z_t | x_<=t, z_<t) (t=1,...,T)\n",
        "        \"\"\"\n",
        "        x_shape = x.shape # (B,C,T)\n",
        "        batch_size = x_shape[0]\n",
        "\n",
        "        z_t_minus = self.initial_input.repeat(batch_size, 1, 1) # (B,C,1)\n",
        "        random_indices = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for t in range(self.n_steps-1):\n",
        "                inputs = torch.cat([x[...,:t+1].detach(), z_t_minus.detach()], dim=1) # (B,C+C,t+1) x_<=t and z_<t\n",
        "                outputs = self.layers(inputs) #(B, C*k, t+1)\n",
        "                q_z_t = outputs[...,-1] # (B, C*k, 1) q(z_t | x_<=t, z_<t)\n",
        "\n",
        "                # sampling from q(z_t | x_<=t, z_<t)\n",
        "                random_index = torch.randint(0, self.k, (batch_size*self.channels,))\n",
        "                random_index += torch.arange(start=0, end=batch_size*self.channels*self.k, step=self.k)\n",
        "                random_index = random_index.to(x.device)\n",
        "                random_indices.append(random_index)\n",
        "\n",
        "                z_t = q_z_t.view(batch_size*self.channels*self.k)[random_index] # (B*C,)\n",
        "                z_t = z_t.view(batch_size, self.channels, 1) #(B,C,1)\n",
        "\n",
        "                z_t_minus = torch.cat([z_t_minus, z_t], dim=-1) # (B,C,t+2)\n",
        "\n",
        "        z_t_minus = z_t_minus.detach() # (B,C,T) z_0,...,z_{T-1}\n",
        "        q_z = self.layers(torch.cat([x, z_t_minus], dim=1)) # (B,C*k,T)\n",
        "\n",
        "        sampled_z = q_z.argmax(dim=2) # (B,C,T)\n",
        "\n",
        "        q_z = q_z.view(batch_size, self.channels, self.k, self.n_steps) # (B,C,k,T)\n",
        "\n",
        "        return sampled_z, q_z"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yY_CEK4dTb0I"
      },
      "source": [
        "## FSVAE prior"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P1r1eqFRTL_v"
      },
      "outputs": [],
      "source": [
        "class PriorBernoulliSTBP(nn.Module):\n",
        "    def __init__(self, latent_dim=20, n_steps=10) -> None:\n",
        "        \"\"\"\n",
        "        modeling of p(z_t|z_<t)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.channels = latent_dim\n",
        "        self.k = 20\n",
        "        self.n_steps = n_steps\n",
        "\n",
        "        self.layers = nn.Sequential(\n",
        "            tdLinear(self.channels,\n",
        "                    self.channels*2,\n",
        "                    bias=True,\n",
        "                    bn=tdBatchNorm(self.channels*2, alpha=2),\n",
        "                    spike=LIFSpike()),\n",
        "            tdLinear(self.channels*2,\n",
        "                    self.channels*4,\n",
        "                    bias=True,\n",
        "                    bn=tdBatchNorm(self.channels*4, alpha=2),\n",
        "                    spike=LIFSpike()),\n",
        "            tdLinear(self.channels*4,\n",
        "                    self.channels*self.k,\n",
        "                    bias=True,\n",
        "                    bn=tdBatchNorm(self.channels*self.k, alpha=2),\n",
        "                    spike=LIFSpike())\n",
        "        )\n",
        "        self.register_buffer('initial_input', torch.zeros(1, self.channels, 1)) # (1,C,1)\n",
        "\n",
        "    def forward(self, z):\n",
        "        \"\"\"\n",
        "        input z: (B,C,T) # latent spike sampled from posterior\n",
        "        output : (B,C,k,T) # indicates p(z_t|z_<t) (t=1,...,T)\n",
        "        \"\"\"\n",
        "        z_shape = z.shape # (B,C,T)\n",
        "        batch_size = z_shape[0]\n",
        "        z = z.detach()\n",
        "\n",
        "        z0 = self.initial_input.repeat(batch_size, 1, 1) # (B,C,1)\n",
        "        inputs = torch.cat([z0, z[...,:-1]], dim=-1) # (B,C,T)\n",
        "        outputs = self.layers(inputs) # (B,C*k,T)\n",
        "\n",
        "        p_z = outputs.view(batch_size, self.channels, self.k, self.n_steps) # (B,C,k,T)\n",
        "        return p_z\n",
        "\n",
        "    def sample(self, batch_size=64):\n",
        "        z_minus_t = self.initial_input.repeat(batch_size, 1, 1) # (B, C, 1)\n",
        "        for t in range(self.n_steps):\n",
        "            outputs = self.layers(z_minus_t) # (B, C*k, t+1)\n",
        "            p_z_t = outputs[...,-1] # (B, C*k, 1)\n",
        "\n",
        "            random_index = torch.randint(0, self.k, (batch_size*self.channels,))\n",
        "            random_index += torch.arange(start=0, end=batch_size*self.channels*self.k, step=self.k)\n",
        "            random_index = random_index.to(z_minus_t.device)\n",
        "\n",
        "            z_t = p_z_t.view(batch_size*self.channels*self.k)[random_index] # (B*C,)\n",
        "            z_t = z_t.view(batch_size, self.channels, 1) #(B,C,1)\n",
        "            z_minus_t = torch.cat([z_minus_t, z_t], dim=-1) # (B,C,t+2)\n",
        "\n",
        "        sampled_z = z_minus_t[...,1:] # (B,C,T)\n",
        "\n",
        "        return sampled_z"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0AAEQLSSHm8"
      },
      "source": [
        "## FSVAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kc07gliPnMK"
      },
      "outputs": [],
      "source": [
        "class FSVAE(nn.Module):\n",
        "    def __init__(self, in_channels, latent_dim, n_steps, k):\n",
        "        super().__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        self.n_steps = n_steps\n",
        "        self.k = k\n",
        "\n",
        "        hidden_dims = [32, 64, 128]\n",
        "\n",
        "        # Build Encoder\n",
        "        modules = []\n",
        "        in_channels = in_channels\n",
        "        is_first_conv = True\n",
        "        for h_dim in hidden_dims:\n",
        "            modules.append(\n",
        "                tdConv(in_channels,\n",
        "                       out_channels=h_dim,\n",
        "                       kernel_size=3,\n",
        "                       stride=2,\n",
        "                       padding=1,\n",
        "                       bias=True,\n",
        "                       bn=tdBatchNorm(h_dim),\n",
        "                       spike=LIFSpike(),\n",
        "                       is_first_conv=is_first_conv)\n",
        "            )\n",
        "            in_channels = h_dim\n",
        "            is_first_conv = False\n",
        "\n",
        "        self.encoder = nn.Sequential(*modules)\n",
        "        self.before_latent_layer = tdLinear(hidden_dims[-1]*25,\n",
        "                                            latent_dim,\n",
        "                                            bias=True,\n",
        "                                            bn=tdBatchNorm(latent_dim),\n",
        "                                            spike=LIFSpike())\n",
        "\n",
        "        self.prior = PriorBernoulliSTBP(self.k)\n",
        "        self.posterior = PosteriorBernoulliSTBP(self.k)\n",
        "\n",
        "        # Build Decoder\n",
        "        modules = []\n",
        "        self.decoder_input = tdLinear(latent_dim,\n",
        "                                      hidden_dims[-1] * 25,\n",
        "                                      bias=True,\n",
        "                                      bn=tdBatchNorm(hidden_dims[-1] * 25),\n",
        "                                      spike=LIFSpike())\n",
        "\n",
        "        hidden_dims.reverse()\n",
        "\n",
        "        for i in range(len(hidden_dims) - 1):\n",
        "            modules.append(\n",
        "                tdConvTranspose(hidden_dims[i],\n",
        "                                hidden_dims[i + 1],\n",
        "                                kernel_size=3,\n",
        "                                stride=2,\n",
        "                                padding=1,\n",
        "                                output_padding=1,\n",
        "                                bias=True,\n",
        "                                bn=tdBatchNorm(hidden_dims[i + 1]),\n",
        "                                spike=LIFSpike())\n",
        "            )\n",
        "        self.decoder = nn.Sequential(*modules)\n",
        "\n",
        "        self.final_layer = nn.Sequential(\n",
        "            tdConvTranspose(hidden_dims[-1],\n",
        "                            hidden_dims[-1],\n",
        "                            kernel_size=3,\n",
        "                            stride=2,\n",
        "                            padding=1,\n",
        "                            output_padding=1,\n",
        "                            bias=True,\n",
        "                            bn=tdBatchNorm(hidden_dims[-1]),\n",
        "                            spike=LIFSpike()),\n",
        "            tdConvTranspose(hidden_dims[-1],\n",
        "                            out_channels=1,\n",
        "                            kernel_size=3,\n",
        "                            padding=1,\n",
        "                            bias=True,\n",
        "                            bn=None,\n",
        "                            spike=None)\n",
        "        )\n",
        "\n",
        "        self.membrane_output_layer = MembraneOutputLayer()\n",
        "        self.psp = PSP()\n",
        "\n",
        "        self.p = 0\n",
        "\n",
        "    def forward(self, x):\n",
        "        sampled_z, q_z, p_z = self.encode(x)\n",
        "        x_recon = self.decode(sampled_z)\n",
        "        return x_recon, q_z, p_z, sampled_z\n",
        "\n",
        "    def encode(self, x):\n",
        "        x = self.encoder(x)  # (N,C,H,W,T)\n",
        "        x = torch.flatten(x, start_dim=1, end_dim=3)  # (N,C*H*W,T)\n",
        "        latent_x = self.before_latent_layer(x)  # (N,latent_dim,T)\n",
        "        sampled_z, q_z = self.posterior(latent_x)\n",
        "        p_z = self.prior(sampled_z)\n",
        "        return sampled_z, q_z, p_z\n",
        "\n",
        "    def decode(self, z):\n",
        "        result = self.decoder_input(z)  # (N,C*H*W,T)\n",
        "        result = result.view(result.shape[0], self.hidden_dims[-1], 5, 5, self.n_steps)  # (N,C,H,W,T)\n",
        "        result = self.decoder(result)  # (N,C,H,W,T)\n",
        "        result = self.final_layer(result)  # (N,C,H,W,T)\n",
        "        out = torch.sigmoid(self.membrane_output_layer(result))\n",
        "        return out\n",
        "\n",
        "    def sample(self, batch_size=64):\n",
        "        sampled_z = self.prior.sample(batch_size)\n",
        "        sampled_x = self.decode(sampled_z)\n",
        "        return sampled_x, sampled_z\n",
        "\n",
        "    def loss_function_mmd(self, input_img, recons_img, q_z, p_z):\n",
        "        \"\"\"\n",
        "        q_z is q(z|x): (N,latent_dim,k,T)\n",
        "        p_z is p(z): (N,latent_dim,k,T)\n",
        "        \"\"\"\n",
        "        recons_loss = F.mse_loss(recons_img, input_img)\n",
        "        q_z_ber = torch.mean(q_z, dim=2)  # (N, latent_dim, T)\n",
        "        p_z_ber = torch.mean(p_z, dim=2)  # (N, latent_dim, T)\n",
        "\n",
        "        mmd_loss = torch.mean((self.psp(q_z_ber) - self.psp(p_z_ber)) ** 2)\n",
        "        loss = recons_loss + mmd_loss\n",
        "        return {'loss': loss, 'Reconstruction_Loss': recons_loss, 'Distance_Loss': mmd_loss}\n",
        "\n",
        "    def weight_clipper(self):\n",
        "        with torch.no_grad():\n",
        "            for p in self.parameters():\n",
        "                p.data.clamp_(-4, 4)\n",
        "\n",
        "    def update_p(self, epoch, max_epoch):\n",
        "        init_p = 0.1\n",
        "        last_p = 0.3\n",
        "        self.p = (last_p - init_p) * epoch / max_epoch + init_p\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJrnB_GaUayF"
      },
      "source": [
        "# Training and Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qxudPr2-0F32"
      },
      "outputs": [],
      "source": [
        "# Create an instance of the FSVAE class\n",
        "network = FSVAE(in_channels, latent_dim, n_steps, k)\n",
        "\n",
        "optimizer = optim.AdamW(network.parameters(),\n",
        "                        lr=lr,\n",
        "                        betas=(0.9, 0.999),\n",
        "                        weight_decay=0.001)\n",
        "\n",
        "# Initialize variables for early stopping\n",
        "best_val_loss = float('inf')\n",
        "patience = 0\n",
        "patience_limit = 10  # Number of epochs without improvement before stopping\n",
        "\n",
        "# Initialize lists to store training and validation losses\n",
        "train_losses_list = []\n",
        "val_losses_list = []\n",
        "\n",
        "# Initialize lists to store evaluation metrics\n",
        "train_metrics_list = []\n",
        "val_metrics_list = []\n",
        "\n",
        "total_start_time = time.time()\n",
        "\n",
        "for epoch in range(start_epoch + 1, epochs):\n",
        "    # Training phase\n",
        "    network.train()\n",
        "    train_loss = 0.0\n",
        "    start_time = time.time()\n",
        "    train_reconstruction = []\n",
        "    train_data = []\n",
        "\n",
        "    # Training loop\n",
        "    for batch_index, data_batch in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        spike_input = data_batch.unsqueeze(-1).repeat(1, 1, 1, 1, n_steps)\n",
        "        x_recon, q_z, p_z, sampled_z = network(spike_input)\n",
        "        losses = network.loss_function_mmd(data_batch, x_recon, q_z, p_z)\n",
        "        losses['loss'].backward()\n",
        "        optimizer.step()\n",
        "        train_loss += losses['loss'].detach().item() * data_batch.size(0)\n",
        "\n",
        "        train_reconstruction.append(x_recon.detach().numpy())\n",
        "        train_data.append(data_batch.detach().numpy())\n",
        "\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "\n",
        "    # Concatenate training data\n",
        "    train_data = np.concatenate(train_data, axis=0)\n",
        "    train_reconstruction = np.concatenate(train_reconstruction, axis=0)\n",
        "    train_reconstruction = np.squeeze(train_reconstruction)\n",
        "\n",
        "    train_data_flat = train_data.reshape(-1, train_data.shape[2] * train_data.shape[3])\n",
        "    train_reconstruction_flat = train_reconstruction.reshape(-1, train_reconstruction.shape[1] * train_reconstruction.shape[2])\n",
        "\n",
        "    # Compute training metrics\n",
        "    train_metrics = {\n",
        "        'MSE': mean_squared_error(train_data_flat, train_reconstruction_flat),\n",
        "        'MAE': mean_absolute_error(train_data_flat, train_reconstruction_flat),\n",
        "        'SSIM': ssim(train_data_flat, train_reconstruction_flat, data_range=train_reconstruction_flat.max() - train_reconstruction_flat.min())\n",
        "    }\n",
        "    train_metrics_list.append(train_metrics)\n",
        "\n",
        "    # Validation phase\n",
        "    network.eval()\n",
        "    val_loss = 0.0\n",
        "    val_reconstruction = []\n",
        "    val_data = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, data_batch in enumerate(val_loader):\n",
        "            spike_input = data_batch.unsqueeze(-1).repeat(1, 1, 1, 1, n_steps)\n",
        "            x_recon, q_z, p_z, sampled_z = network(spike_input)\n",
        "            val_losses = network.loss_function_mmd(data_batch, x_recon, q_z, p_z)\n",
        "            val_loss += val_losses['loss'].detach().item() * data_batch.size(0)\n",
        "\n",
        "            val_reconstruction.append(x_recon.detach().numpy())\n",
        "            val_data.append(data_batch.detach().numpy())\n",
        "\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "\n",
        "    end_time = time.time()\n",
        "    epoch_time = end_time - start_time\n",
        "    print(f\"Epoch [{epoch}/{epochs}], Time: {epoch_time:.2f} seconds\")\n",
        "\n",
        "    # Concatenate validation data\n",
        "    val_data = np.concatenate(val_data, axis=0)\n",
        "    val_reconstruction = np.concatenate(val_reconstruction, axis=0)\n",
        "    val_reconstruction = np.squeeze(val_reconstruction)\n",
        "\n",
        "    val_data_flat = val_data.reshape(-1, val_data.shape[2] * val_data.shape[3])\n",
        "    val_reconstruction_flat = val_reconstruction.reshape(-1, val_reconstruction.shape[1] * val_reconstruction.shape[2])\n",
        "\n",
        "    # Compute validation metrics\n",
        "    val_metrics = {\n",
        "        'MSE': mean_squared_error(val_data_flat, val_reconstruction_flat),\n",
        "        'MAE': mean_absolute_error(val_data_flat, val_reconstruction_flat),\n",
        "        'SSIM': ssim(val_data_flat, val_reconstruction_flat, data_range=val_reconstruction_flat.max() - val_reconstruction_flat.min())\n",
        "    }\n",
        "    val_metrics_list.append(val_metrics)\n",
        "\n",
        "    print(f\"Epoch [{epoch}/{epochs}], Train Loss: {train_loss}, Val Loss: {val_loss}\")\n",
        "    print(f\"Epoch [{epoch}/{epochs}], Train MSE: {train_metrics['MSE']}, Train MAE: {train_metrics['MAE']}, Train SSIM: {train_metrics['SSIM']}\")\n",
        "    print(f\"Epoch [{epoch}/{epochs}], Val MSE: {val_metrics['MSE']}, Val MAE: {val_metrics['MAE']}, Val SSIM: {val_metrics['SSIM']}\")\n",
        "\n",
        "    # Save losses to lists\n",
        "    train_losses_list.append(train_loss)\n",
        "    val_losses_list.append(val_loss)\n",
        "\n",
        "    # Save model checkpoint after every epoch\n",
        "    save_checkpoint(epoch, network, optimizer, best_val_loss, f'model_name_{epoch}.pth')\n",
        "\n",
        "    # Save losses as CSV after every epoch\n",
        "    csv_path = os.path.join(save_folder, 'file_name.csv')\n",
        "    losses_df = pd.DataFrame({\n",
        "        'Epoch': range(start_epoch + 1, epoch + 1),\n",
        "        'Train Loss': train_losses_list,\n",
        "        'Validation Loss': val_losses_list,\n",
        "        'Train MSE': [metrics['MSE'] for metrics in train_metrics_list],\n",
        "        'Train MAE': [metrics['MAE'] for metrics in train_metrics_list],\n",
        "        'Train SSIM': [metrics['SSIM'] for metrics in train_metrics_list],\n",
        "        'Validation MSE': [metrics['MSE'] for metrics in val_metrics_list],\n",
        "        'Validation MAE': [metrics['MAE'] for metrics in val_metrics_list],\n",
        "        'Validation SSIM': [metrics['SSIM'] for metrics in val_metrics_list]\n",
        "    })\n",
        "    losses_df.to_csv(csv_path, index=False)\n",
        "\n",
        "    # Check for improvement in validation loss\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        patience = 0\n",
        "        # Save the model to the specified folder\n",
        "        model_path = os.path.join(save_folder, 'best_model.pth')\n",
        "        torch.save(network.state_dict(), model_path)\n",
        "    else:\n",
        "        patience += 1\n",
        "\n",
        "    # Early stopping\n",
        "    if patience > patience_limit:\n",
        "        print(\"Early stopping! No improvement in validation loss.\")\n",
        "        break\n",
        "\n",
        "# Calculate time\n",
        "total_end_time = time.time()\n",
        "epoch_time = total_end_time - total_start_time\n",
        "print(f\"Total Time: {epoch_time:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "Iz16jeSZHfyD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zX25nvSxkxNG"
      },
      "outputs": [],
      "source": [
        "# Load the model\n",
        "network = FSVAE(in_channels, latent_dim, n_steps, k)\n",
        "checkpoint = torch.load(model_path)\n",
        "network.load_state_dict(checkpoint)\n",
        "\n",
        "# Evaluation\n",
        "reconstruction = []\n",
        "data_test = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_idx, data_batch in enumerate(test_loader):\n",
        "        spike_input = data_batch.unsqueeze(-1).repeat(1, 1, 1, 1, n_steps)\n",
        "        x_recon, q_z, p_z, sampled_z = network(spike_input)\n",
        "\n",
        "        reconstruction.append(x_recon.numpy())\n",
        "        data_test.append(data_batch.numpy())\n",
        "\n",
        "# Concatenate data_test\n",
        "data_test = np.concatenate(data_test, axis=0)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "reconstruction = np.concatenate(reconstruction, axis=0)\n",
        "reconstruction = np.squeeze(reconstruction)\n",
        "\n",
        "data_test_flat = data_test.reshape(-1, data_test.shape[2] * data_test.shape[3])\n",
        "reconstruction_flat = reconstruction.reshape(-1, reconstruction.shape[1] * reconstruction.shape[2])\n",
        "\n",
        "mse = mean_squared_error(data_test_flat, reconstruction_flat)\n",
        "mae = mean_absolute_error(data_test_flat, reconstruction_flat)\n",
        "ssim_score = ssim(data_test_flat, reconstruction_flat,\n",
        "                  data_range=reconstruction_flat.max() - reconstruction_flat.min())\n",
        "\n",
        "print(f\"Mean Squared Error: {mse}\")\n",
        "print(f\"Mean Absolute Error: {mae}\")\n",
        "print(f\"SSIM: {ssim_score}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualisation"
      ],
      "metadata": {
        "id": "tC2qzMy_Hhqv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YXXvv16VoogB"
      },
      "outputs": [],
      "source": [
        "# Visualize actual and predicted images with sample indices\n",
        "n_samples = 5  # Number of samples to visualize\n",
        "\n",
        "# Select random samples\n",
        "available_indices = list(range(len(data_test)))\n",
        "indices = np.random.choice(available_indices, size=n_samples, replace=False)\n",
        "\n",
        "# Plot actual and predicted images\n",
        "fig, axes = plt.subplots(nrows=2, ncols=n_samples, figsize=(15, 5))\n",
        "\n",
        "for i, idx in enumerate(indices):\n",
        "    # Remove selected index from available indices\n",
        "    available_indices.remove(idx)\n",
        "\n",
        "    # Find maximum and minimum pixel values for the actual image\n",
        "    max_pixel_value_actual = data_test[idx].max()\n",
        "    min_pixel_value_actual = data_test[idx].min()\n",
        "\n",
        "    # Plot actual image with index\n",
        "    axes[0, i].imshow(data_test[idx].squeeze(), cmap='jet')\n",
        "    axes[0, i].set_title(f\"Actual ({idx})\")\n",
        "    axes[0, i].axis('off')\n",
        "\n",
        "    # Plot predicted image with index using actual image's max and min pixel values\n",
        "    reconstructed_img = reconstruction[idx]\n",
        "    axes[1, i].imshow(reconstructed_img, cmap='jet')\n",
        "    axes[1, i].set_title(f\"Predicted ({idx})\")\n",
        "    axes[1, i].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "img_path = os.path.join(save_folder, 'svae3_10000_35_1.png')\n",
        "plt.savefig(img_path)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXwpQUa6DWqB"
      },
      "outputs": [],
      "source": [
        "# Visualize actual and predicted images with sample indices\n",
        "n_samples = 5  # Number of samples to visualize\n",
        "\n",
        "# Select random samples\n",
        "available_indices = list(range(len(data_test)))\n",
        "indices = np.random.choice(available_indices, size=n_samples, replace=False)\n",
        "\n",
        "# Plot actual and predicted images\n",
        "fig, axes = plt.subplots(nrows=2, ncols=n_samples, figsize=(15, 5))\n",
        "\n",
        "for i, idx in enumerate(indices):\n",
        "    # Remove selected index from available indices\n",
        "    available_indices.remove(idx)\n",
        "\n",
        "    # Find maximum and minimum pixel values for the actual image\n",
        "    max_pixel_value_actual = data_test[idx].max()\n",
        "    min_pixel_value_actual = data_test[idx].min()\n",
        "\n",
        "    # Plot actual image with index\n",
        "    axes[0, i].imshow(data_test[idx].squeeze(), cmap='jet')\n",
        "    axes[0, i].set_title(f\"Actual ({idx})\")\n",
        "    axes[0, i].axis('off')\n",
        "\n",
        "    # Plot predicted image with index using actual image's max and min pixel values\n",
        "    reconstructed_img = reconstruction[idx]\n",
        "    # reconstructed_img_clipped = np.clip(reconstructed_img, min_pixel_value_actual, max_pixel_value_actual)\n",
        "    axes[1, i].imshow(reconstructed_img, cmap='jet')\n",
        "    axes[1, i].set_title(f\"Predicted ({idx})\")\n",
        "    axes[1, i].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "img_path = os.path.join(save_folder, 'svae3_10000_35_2.png')\n",
        "plt.savefig(img_path)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}